{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: the md5 module is deprecated; use hashlib instead\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "import uuid\n",
    "from string import lowercase as alphabet\n",
    "import re\n",
    "import md5\n",
    "import cPickle as pickle\n",
    "import alchemy\n",
    "from collections import Counter\n",
    "from Levenshtein import jaro_winkler\n",
    "from alchemy import get_config, match\n",
    "from alchemy.schema import *\n",
    "from alchemy.match import commit_inserts, commit_updates\n",
    "from handlers.xml_util import normalize_utf8\n",
    "from datetime import datetime\n",
    "from sqlalchemy.sql import or_\n",
    "from sqlalchemy.sql.expression import bindparam\n",
    "from unidecode import unidecode\n",
    "from tasks import bulk_commit_inserts, bulk_commit_updates\n",
    "import sys\n",
    "config = get_config()\n",
    "\n",
    "THRESHOLD = config.get(\"lawyer\").get(\"threshold\")\n",
    "\n",
    "# bookkeeping for blocks\n",
    "blocks = defaultdict(list)\n",
    "id_map = defaultdict(list)\n",
    "\n",
    "nodigits = re.compile(r'[^\\d]+')\n",
    "\n",
    "lawyer_dict = {}\n",
    "\n",
    "lawyer_insert_statements = []\n",
    "patentlawyer_insert_statements = []\n",
    "update_statements = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lawyer_id(obj):\n",
    "    \"\"\"\n",
    "    Returns string representing an lawyer object. Returns obj.organization if\n",
    "    it exists, else returns concatenated obj.name_first + '|' + obj.name_last\n",
    "    \"\"\"\n",
    "    if obj.organization:\n",
    "        return obj.organization\n",
    "    try:\n",
    "        return obj.name_first + '|' + obj.name_last\n",
    "    except:\n",
    "        return ''\n",
    "    \n",
    "def clean_lawyers(list_of_lawyers):\n",
    "    \"\"\"\n",
    "    Removes the following stop words from each lawyer:\n",
    "    the, of, and, a, an, at\n",
    "    Then, blocks the lawyer with other lawyers that start\n",
    "    with the same letter. Returns a list of these blocks\n",
    "    \"\"\"\n",
    "    stoplist = ['the', 'of', 'and', 'a', 'an', 'at']\n",
    "    #alpha_blocks = defaultdict(list)\n",
    "    block = []\n",
    "    print 'Removing stop words, blocking by first letter...'\n",
    "    for lawyer in list_of_lawyers:\n",
    "        lawyer_dict[lawyer.uuid] = lawyer\n",
    "        a_id = get_lawyer_id(lawyer)\n",
    "        # removes stop words, then rejoins the string\n",
    "        a_id = ' '.join(filter(lambda x:\n",
    "                            x.lower() not in stoplist,\n",
    "                            a_id.split(' ')))\n",
    "        a_id = ''.join(nodigits.findall(a_id)).strip()\n",
    "        id_map[a_id].append(lawyer.uuid)\n",
    "        block.append(a_id)\n",
    "    print 'lawyers cleaned!'\n",
    "    return block\n",
    "def without_digits(word):\n",
    "    return ''.join([x for x in word if not x.isdigit()])\n",
    "\n",
    "def create_jw_blocks(list_of_lawyers):\n",
    "    \"\"\"\n",
    "    Receives list of blocks, where a block is a list of lawyers\n",
    "    that all begin with the same letter. Within each block, does\n",
    "    a pairwise jaro winkler comparison to block lawyers together\n",
    "    \"\"\"\n",
    "    global blocks\n",
    "    consumed = defaultdict(int)\n",
    "    print 'Doing pairwise Jaro-Winkler...', len(list_of_lawyers)\n",
    "    for i, primary in enumerate(list_of_lawyers):\n",
    "        if consumed[primary]: continue\n",
    "        consumed[primary] = 1\n",
    "        blocks[primary].append(primary)\n",
    "        for secondary in list_of_lawyers[i:]:\n",
    "            if consumed[secondary]: continue\n",
    "            if primary == secondary:\n",
    "                blocks[primary].append(secondary)\n",
    "                continue\n",
    "            if jaro_winkler(primary, secondary, 0.0) >= THRESHOLD:\n",
    "                consumed[secondary] = 1\n",
    "                blocks[primary].append(secondary)\n",
    "    pickle.dump(blocks, open('lawyer.pickle', 'wb'))\n",
    "    print 'lawyer blocks created!'\n",
    "    #added\n",
    "    return blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_lawyer_table(session):\n",
    "    \"\"\"\n",
    "    Given a list of lawyers and the redis key-value disambiguation,\n",
    "    populates the lawyer table in the database\n",
    "    \"\"\"\n",
    "    print 'Disambiguating lawyers...'\n",
    "    if alchemy.is_mysql():\n",
    "        session.execute('set foreign_key_checks = 0;')\n",
    "        session.commit()\n",
    "    i = 0\n",
    "    for lawyer in blocks.iterkeys():\n",
    "        ra_ids = (id_map[ra] for ra in blocks[lawyer])\n",
    "        for block in ra_ids:\n",
    "            i += 1\n",
    "            rawlawyers = [lawyer_dict[ra_id] for ra_id in block]\n",
    "            if i % 20000 == 0:\n",
    "                print i, datetime.now()\n",
    "                lawyer_match(rawlawyers, session, commit=True)\n",
    "            else:\n",
    "                lawyer_match(rawlawyers, session, commit=False)\n",
    "    # temporarily comment out these three lines and see if I can bulk commit at the end?\n",
    "    bulk_commit_inserts(lawyer_insert_statements, Lawyer.__table__, alchemy.is_mysql(), 20000, 'grant')\n",
    "    bulk_commit_inserts(patentlawyer_insert_statements, patentlawyer, alchemy.is_mysql(), 20000)\n",
    "    bulk_commit_updates('lawyer_id', update_statements, RawLawyer.__table__, alchemy.is_mysql(), 20000)\n",
    "    # t1.get()\n",
    "    # t2.get()\n",
    "    # t3.get()\n",
    "    # session.commit()\n",
    "    print i, datetime.now()\n",
    "\n",
    "def lawyer_match(objects, session, commit=False):\n",
    "    freq = defaultdict(Counter)\n",
    "    param = {}\n",
    "    raw_objects = []\n",
    "    clean_objects = []\n",
    "    clean_cnt = 0\n",
    "    clean_main = None\n",
    "    class_type = None\n",
    "    class_type = None\n",
    "    for obj in objects:\n",
    "        if not obj: continue\n",
    "        class_type = obj.__related__\n",
    "        raw_objects.append(obj)\n",
    "        break\n",
    "\n",
    "    param = {}\n",
    "    for obj in raw_objects:\n",
    "        for k, v in obj.summarize.iteritems():\n",
    "            freq[k][v] += 1\n",
    "        if \"id\" not in param:\n",
    "            param[\"id\"] = obj.uuid\n",
    "        param[\"id\"] = min(param[\"id\"], obj.uuid)\n",
    "\n",
    "    # create parameters based on most frequent\n",
    "    for k in freq:\n",
    "        if None in freq[k]:\n",
    "            freq[k].pop(None)\n",
    "        if \"\" in freq[k]:\n",
    "            freq[k].pop(\"\")\n",
    "        if freq[k]:\n",
    "            param[k] = freq[k].most_common(1)[0][0]\n",
    "    if not param.has_key('organization'):\n",
    "        param['organization'] = ''\n",
    "    if not param.has_key('type'):\n",
    "        param['type'] = ''\n",
    "    if not param.has_key('name_last'):\n",
    "        param['name_last'] = ''\n",
    "    if not param.has_key('name_first'):\n",
    "        param['name_first'] = ''\n",
    "    if not param.has_key('residence'):\n",
    "        param['residence'] = ''\n",
    "    if not param.has_key('nationality'):\n",
    "        param['nationality'] = ''\n",
    "    if not param.has_key('country'):\n",
    "        param['country'] = ''\n",
    "\n",
    "    if param[\"organization\"]:\n",
    "        param[\"id\"] = md5.md5(unidecode(param[\"organization\"])).hexdigest()\n",
    "    if param[\"name_last\"]:\n",
    "        param[\"id\"] = md5.md5(unidecode(param[\"name_last\"]+param[\"name_first\"])).hexdigest()\n",
    "    \n",
    "    lawyer_insert_statements.append(param)\n",
    "    tmpids = map(lambda x: x.uuid, objects)\n",
    "    patents = map(lambda x: x.patent_id, objects)\n",
    "    patentlawyer_insert_statements.extend([{'patent_id': x, 'lawyer_id': param['id']} for x in patents])\n",
    "    update_statements.extend([{'pk':x,'update':param['id']} for x in tmpids])\n",
    "\n",
    "def examine():\n",
    "    lawyers = s.query(lawyer).all()\n",
    "    for a in lawyers:\n",
    "        print get_lawyer_id(a), len(a.rawlawyers)\n",
    "        for ra in a.rawlawyers:\n",
    "            if get_lawyer_id(ra) != get_lawyer_id(a):\n",
    "                print get_lawyer_id(ra)\n",
    "            print '-'*10\n",
    "    print len(lawyers)\n",
    "\n",
    "\n",
    "def printall():\n",
    "    lawyers = s.query(lawyer).all()\n",
    "    with open('out.txt', 'wb') as f:\n",
    "        for a in lawyers:\n",
    "            f.write(normalize_utf8(get_lawyer_id(a)).encode('utf-8'))\n",
    "            f.write('\\n')\n",
    "            for ra in a.rawlawyers:\n",
    "                f.write(normalize_utf8(get_lawyer_id(ra)).encode('utf-8'))\n",
    "                f.write('\\n')\n",
    "            f.write('-'*20)\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def run_letter(letter, session, doctype='grant'):\n",
    "    schema = RawLawyer\n",
    "    if doctype == 'application':\n",
    "        schema = App_RawLawyer\n",
    "    letter = letter.upper()\n",
    "    clause1 = schema.organization.startswith(bindparam('letter',letter))\n",
    "    clause2 = schema.name_first.startswith(bindparam('letter',letter))\n",
    "    clauses = or_(clause1, clause2)\n",
    "    lawyers = (lawyer for lawyer in session.query(schema).filter(clauses))\n",
    "    block = clean_lawyers(lawyers)\n",
    "    create_jw_blocks(block)\n",
    "    create_lawyer_table(session)\n",
    "\n",
    "\n",
    "def run_disambiguation(doctype='grant'):\n",
    "    # get all lawyers in database\n",
    "    global blocks\n",
    "    global lawyer_insert_statements\n",
    "    global patentlawyer_insert_statements\n",
    "    global update_statements\n",
    "    session = alchemy.fetch_session(dbtype=doctype)\n",
    "    if doctype == 'grant':\n",
    "        lawyers = deque(session.query(RawLawyer))\n",
    "    if doctype == 'application':\n",
    "        lawyers = deque(session.query(App_RawLawyer))\n",
    "    lawyer_alpha_blocks = clean_lawyers(lawyers)\n",
    "    lawyer_insert_statements = []\n",
    "    patentlawyer_insert_statements = []\n",
    "    update_statements = []\n",
    "    for letter in alphabet:\n",
    "        print letter, datetime.now()\n",
    "        blocks = defaultdict(list)\n",
    "        lawyer_insert_statements = []\n",
    "        patentlawyer_insert_statements = []\n",
    "        update_statements = []\n",
    "        letterblock = [x for x in lawyer_alpha_blocks if x.lower().startswith(letter)]\n",
    "        create_jw_blocks(letterblock)\n",
    "        create_lawyer_table(session)\n",
    "    print len(lawyer_insert_statements)\n",
    "    print len(update_statements)\n",
    "    bulk_commit_inserts(lawyer_insert_statements, Lawyer.__table__, alchemy.is_mysql(), 20000, 'grant')\n",
    "    bulk_commit_inserts(patentlawyer_insert_statements, patentlawyer, alchemy.is_mysql(), 20000)\n",
    "    bulk_commit_updates('lawyer_id', update_statements, RawLawyer.__table__, alchemy.is_mysql(), 20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lawyer disambiguation debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_disambiguation(doctype='grant'):\n",
    "    # get all lawyers in database\n",
    "    global blocks\n",
    "    global lawyer_insert_statements\n",
    "    global patentlawyer_insert_statements\n",
    "    global update_statements\n",
    "    session = alchemy.fetch_session(dbtype=doctype)\n",
    "    if doctype == 'grant':\n",
    "        lawyers = deque(session.query(RawLawyer))\n",
    "    if doctype == 'application':\n",
    "        lawyers = deque(session.query(App_RawLawyer))\n",
    "    print len(lawyers)\n",
    "    return lawyers\n",
    "def get_alpha_blocks(lawyer_deque):\n",
    "    lawyer_alpha_blocks = clean_lawyers(lawyer_deque)\n",
    "    return lawyer_alpha_blocks\n",
    "def do_jw(lawyer_alpha_blocks):\n",
    "    session = alchemy.fetch_session(dbtype='grant')\n",
    "    for letter in ['w', 'y']:\n",
    "        print letter, datetime.now()\n",
    "        blocks = defaultdict(list)\n",
    "        lawyer_insert_statements = []\n",
    "        patentlawyer_insert_statements = []\n",
    "        update_statements = []\n",
    "        letterblock = [x for x in lawyer_alpha_blocks if x.lower().startswith(letter)]\n",
    "        results = create_jw_blocks(letterblock)\n",
    "        create_lawyer_table(session)\n",
    "        \n",
    "        #results is an id to name making that makes sense\n",
    "        \n",
    "    #return results, lawyer_insert_statements, update_statements, patentlawyer_insert_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380035\n"
     ]
    }
   ],
   "source": [
    "lawyer_ex = run_disambiguation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words, blocking by first letter...\n",
      "lawyers cleaned!\n"
     ]
    }
   ],
   "source": [
    "lawyer_alpha_block = get_alpha_blocks(lawyer_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w 2018-02-13 11:02:21.470000\n",
      "Doing pairwise Jaro-Winkler... 446769\n",
      "lawyer blocks created!\n",
      "Disambiguating lawyers...\n",
      "committing last 8927 records at 2018-02-13 11:13:37.781000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '4a00dfc7b6e92feabb2a5c9c8...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '7e8cf3cfc86d2d744187acd60...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value 'a361386891f3497d21c645103...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value 'd9e88418015815d7d91e8e009...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '46e11901bb0fe741fe3609d10...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '59cc06f22c74f9620f8a7b021...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '48681d9ab6e9c6e0fb6aa9de8...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value 'a4eabffcb1f8965d128204053...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value '3aa7e7c4fdf732cca63881cd0...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n",
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sqlalchemy\\sql\\sqltypes.py:201: SAWarning: Unicode type received non-unicode bind param value 'f38504e2d61adf93ad756eeb7...'. (this warning may be suppressed after 10 occurrences)\n",
      "  (util.ellipses_string(value),))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last chunk in, committing\n",
      "committing chunk 1 of 22 with length 20000 at 2018-02-13 11:13:44.231000\n",
      "committing chunk 2 of 22 with length 20000 at 2018-02-13 11:13:45.576000\n",
      "committing chunk 3 of 22 with length 20000 at 2018-02-13 11:13:46.725000\n",
      "committing chunk 4 of 22 with length 20000 at 2018-02-13 11:13:47.821000\n",
      "committing chunk 5 of 22 with length 20000 at 2018-02-13 11:13:48.863000\n",
      "committing chunk 6 of 22 with length 20000 at 2018-02-13 11:13:49.990000\n",
      "committing chunk 7 of 22 with length 20000 at 2018-02-13 11:13:51.167000\n",
      "committing chunk 8 of 22 with length 20000 at 2018-02-13 11:13:52.305000\n",
      "committing chunk 9 of 22 with length 20000 at 2018-02-13 11:13:53.428000\n",
      "committing chunk 10 of 22 with length 20000 at 2018-02-13 11:13:54.518000\n",
      "committing chunk 11 of 22 with length 20000 at 2018-02-13 11:13:55.601000\n",
      "committing chunk 12 of 22 with length 20000 at 2018-02-13 11:13:56.711000\n",
      "committing chunk 13 of 22 with length 20000 at 2018-02-13 11:13:57.698000\n",
      "committing chunk 14 of 22 with length 20000 at 2018-02-13 11:13:59.037000\n",
      "committing chunk 15 of 22 with length 20000 at 2018-02-13 11:14:00.204000\n",
      "committing chunk 16 of 22 with length 20000 at 2018-02-13 11:14:01.263000\n",
      "committing chunk 17 of 22 with length 20000 at 2018-02-13 11:14:02.293000\n",
      "committing chunk 18 of 22 with length 20000 at 2018-02-13 11:14:03.329000\n",
      "committing chunk 19 of 22 with length 20000 at 2018-02-13 11:14:04.969000\n",
      "committing chunk 20 of 22 with length 20000 at 2018-02-13 11:14:06.112000\n",
      "committing chunk 21 of 22 with length 20000 at 2018-02-13 11:14:07.293000\n",
      "committing chunk 22 of 22 with length 20000 at 2018-02-13 11:14:08.607000\n",
      "committing last 6769 records at 2018-02-13 11:14:08.619000\n",
      "last chunk in, committing\n",
      "committing chunk 1 of 44 with length 10000 at 2018-02-13 11:14:14.001000\n",
      "committing chunk 2 of 44 with length 10000 at 2018-02-13 11:14:14.315000\n",
      "committing chunk 3 of 44 with length 10000 at 2018-02-13 11:14:14.662000\n",
      "committing chunk 4 of 44 with length 10000 at 2018-02-13 11:14:14.969000\n",
      "committing chunk 5 of 44 with length 10000 at 2018-02-13 11:14:15.271000\n",
      "committing chunk 6 of 44 with length 10000 at 2018-02-13 11:14:15.571000\n",
      "committing chunk 7 of 44 with length 10000 at 2018-02-13 11:14:15.884000\n",
      "committing chunk 8 of 44 with length 10000 at 2018-02-13 11:14:16.496000\n",
      "committing chunk 9 of 44 with length 10000 at 2018-02-13 11:14:16.867000\n",
      "committing chunk 10 of 44 with length 10000 at 2018-02-13 11:14:17.207000\n",
      "committing chunk 11 of 44 with length 10000 at 2018-02-13 11:14:17.961000\n",
      "committing chunk 12 of 44 with length 10000 at 2018-02-13 11:14:18.293000\n",
      "committing chunk 13 of 44 with length 10000 at 2018-02-13 11:14:54.534000\n",
      "committing chunk 14 of 44 with length 10000 at 2018-02-13 11:14:54.904000\n",
      "committing chunk 15 of 44 with length 10000 at 2018-02-13 11:14:55.258000\n",
      "committing chunk 16 of 44 with length 10000 at 2018-02-13 11:14:55.655000\n",
      "committing chunk 17 of 44 with length 10000 at 2018-02-13 11:14:56.044000\n",
      "committing chunk 18 of 44 with length 10000 at 2018-02-13 11:14:56.385000\n",
      "committing chunk 19 of 44 with length 10000 at 2018-02-13 11:14:56.725000\n",
      "committing chunk 20 of 44 with length 10000 at 2018-02-13 11:14:57.098000\n",
      "committing chunk 21 of 44 with length 10000 at 2018-02-13 11:14:57.429000\n",
      "committing chunk 22 of 44 with length 10000 at 2018-02-13 11:14:58.068000\n",
      "committing chunk 23 of 44 with length 10000 at 2018-02-13 11:14:58.460000\n",
      "committing chunk 24 of 44 with length 10000 at 2018-02-13 11:14:58.857000\n",
      "committing chunk 25 of 44 with length 10000 at 2018-02-13 11:14:59.230000\n",
      "committing chunk 26 of 44 with length 10000 at 2018-02-13 11:14:59.603000\n",
      "committing chunk 27 of 44 with length 10000 at 2018-02-13 11:14:59.969000\n",
      "committing chunk 28 of 44 with length 10000 at 2018-02-13 11:15:00.308000\n",
      "committing chunk 29 of 44 with length 10000 at 2018-02-13 11:15:00.662000\n",
      "committing chunk 30 of 44 with length 10000 at 2018-02-13 11:15:01.009000\n",
      "committing chunk 31 of 44 with length 10000 at 2018-02-13 11:15:01.353000\n",
      "committing chunk 32 of 44 with length 10000 at 2018-02-13 11:15:01.720000\n",
      "committing chunk 33 of 44 with length 10000 at 2018-02-13 11:15:02.071000\n",
      "committing chunk 34 of 44 with length 10000 at 2018-02-13 11:15:02.430000\n",
      "committing chunk 35 of 44 with length 10000 at 2018-02-13 11:15:02.813000\n",
      "committing chunk 36 of 44 with length 10000 at 2018-02-13 11:15:03.190000\n",
      "committing chunk 37 of 44 with length 10000 at 2018-02-13 11:15:03.543000\n",
      "committing chunk 38 of 44 with length 10000 at 2018-02-13 11:15:03.907000\n",
      "committing chunk 39 of 44 with length 10000 at 2018-02-13 11:15:04.235000\n",
      "committing chunk 40 of 44 with length 10000 at 2018-02-13 11:15:04.583000\n",
      "committing chunk 41 of 44 with length 10000 at 2018-02-13 11:15:04.961000\n",
      "committing chunk 42 of 44 with length 10000 at 2018-02-13 11:15:05.311000\n",
      "committing chunk 43 of 44 with length 10000 at 2018-02-13 11:15:05.680000\n",
      "committing chunk 44 of 44 with length 10000 at 2018-02-13 11:15:06.036000\n",
      "committing last 6769 records at 2018-02-13 11:15:06.046000\n",
      "last chunk in, committing\n",
      "8927 2018-02-13 11:15:46.100000\n",
      "y 2018-02-13 11:15:46.100000\n",
      "Doing pairwise Jaro-Winkler... 44687\n",
      "lawyer blocks created!\n",
      "Disambiguating lawyers...\n",
      "committing last 18482 records at 2018-02-13 11:16:10.581000\n",
      "last chunk in, committing\n",
      "committing chunk 1 of 46 with length 20000 at 2018-02-13 11:16:20.405000\n",
      "committing chunk 2 of 46 with length 20000 at 2018-02-13 11:16:21.341000\n",
      "committing chunk 3 of 46 with length 20000 at 2018-02-13 11:16:22.230000\n",
      "committing chunk 4 of 46 with length 20000 at 2018-02-13 11:16:23.118000\n",
      "committing chunk 5 of 46 with length 20000 at 2018-02-13 11:16:24.037000\n",
      "committing chunk 6 of 46 with length 20000 at 2018-02-13 11:16:24.966000\n",
      "committing chunk 7 of 46 with length 20000 at 2018-02-13 11:16:25.939000\n",
      "committing chunk 8 of 46 with length 20000 at 2018-02-13 11:16:26.930000\n",
      "committing chunk 9 of 46 with length 20000 at 2018-02-13 11:16:27.956000\n",
      "committing chunk 10 of 46 with length 20000 at 2018-02-13 11:16:29.028000\n",
      "committing chunk 11 of 46 with length 20000 at 2018-02-13 11:16:30.049000\n",
      "committing chunk 12 of 46 with length 20000 at 2018-02-13 11:16:31.152000\n",
      "committing chunk 13 of 46 with length 20000 at 2018-02-13 11:16:32.126000\n",
      "committing chunk 14 of 46 with length 20000 at 2018-02-13 11:16:33.079000\n",
      "committing chunk 15 of 46 with length 20000 at 2018-02-13 11:16:34.095000\n",
      "committing chunk 16 of 46 with length 20000 at 2018-02-13 11:16:35.052000\n",
      "committing chunk 17 of 46 with length 20000 at 2018-02-13 11:16:36.206000\n",
      "committing chunk 18 of 46 with length 20000 at 2018-02-13 11:16:37.295000\n",
      "committing chunk 19 of 46 with length 20000 at 2018-02-13 11:16:38.265000\n",
      "committing chunk 20 of 46 with length 20000 at 2018-02-13 11:16:39.299000\n",
      "committing chunk 21 of 46 with length 20000 at 2018-02-13 11:16:40.275000\n",
      "committing chunk 22 of 46 with length 20000 at 2018-02-13 11:16:41.328000\n",
      "committing chunk 23 of 46 with length 20000 at 2018-02-13 11:16:42.396000\n",
      "committing chunk 24 of 46 with length 20000 at 2018-02-13 11:16:43.480000\n",
      "committing chunk 25 of 46 with length 20000 at 2018-02-13 11:16:44.574000\n",
      "committing chunk 26 of 46 with length 20000 at 2018-02-13 11:16:45.719000\n",
      "committing chunk 27 of 46 with length 20000 at 2018-02-13 11:16:46.658000\n",
      "committing chunk 28 of 46 with length 20000 at 2018-02-13 11:16:47.632000\n",
      "committing chunk 29 of 46 with length 20000 at 2018-02-13 11:16:48.669000\n",
      "committing chunk 30 of 46 with length 20000 at 2018-02-13 11:16:49.817000\n",
      "committing chunk 31 of 46 with length 20000 at 2018-02-13 11:16:50.786000\n",
      "committing chunk 32 of 46 with length 20000 at 2018-02-13 11:16:51.889000\n",
      "committing chunk 33 of 46 with length 20000 at 2018-02-13 11:16:52.732000\n",
      "committing chunk 34 of 46 with length 20000 at 2018-02-13 11:16:53.649000\n",
      "committing chunk 35 of 46 with length 20000 at 2018-02-13 11:16:54.627000\n",
      "committing chunk 36 of 46 with length 20000 at 2018-02-13 11:16:55.630000\n",
      "committing chunk 37 of 46 with length 20000 at 2018-02-13 11:16:56.571000\n",
      "committing chunk 38 of 46 with length 20000 at 2018-02-13 11:16:57.544000\n",
      "committing chunk 39 of 46 with length 20000 at 2018-02-13 11:16:58.336000\n",
      "committing chunk 40 of 46 with length 20000 at 2018-02-13 11:16:59.304000\n",
      "committing chunk 41 of 46 with length 20000 at 2018-02-13 11:17:00.326000\n",
      "committing chunk 42 of 46 with length 20000 at 2018-02-13 11:17:01.365000\n",
      "committing chunk 43 of 46 with length 20000 at 2018-02-13 11:17:02.424000\n",
      "committing chunk 44 of 46 with length 20000 at 2018-02-13 11:17:03.437000\n",
      "committing chunk 45 of 46 with length 20000 at 2018-02-13 11:17:04.491000\n",
      "committing chunk 46 of 46 with length 20000 at 2018-02-13 11:17:05.625000\n",
      "committing last 18225 records at 2018-02-13 11:17:05.638000\n",
      "last chunk in, committing\n",
      "committing chunk 1 of 93 with length 10000 at 2018-02-13 11:17:11.089000\n",
      "committing chunk 2 of 93 with length 10000 at 2018-02-13 11:17:11.390000\n",
      "committing chunk 3 of 93 with length 10000 at 2018-02-13 11:17:11.735000\n",
      "committing chunk 4 of 93 with length 10000 at 2018-02-13 11:17:12.059000\n",
      "committing chunk 5 of 93 with length 10000 at 2018-02-13 11:17:12.366000\n",
      "committing chunk 6 of 93 with length 10000 at 2018-02-13 11:17:12.675000\n",
      "committing chunk 7 of 93 with length 10000 at 2018-02-13 11:17:13.003000\n",
      "committing chunk 8 of 93 with length 10000 at 2018-02-13 11:17:13.337000\n",
      "committing chunk 9 of 93 with length 10000 at 2018-02-13 11:17:13.673000\n",
      "committing chunk 10 of 93 with length 10000 at 2018-02-13 11:17:13.988000\n",
      "committing chunk 11 of 93 with length 10000 at 2018-02-13 11:17:14.288000\n",
      "committing chunk 12 of 93 with length 10000 at 2018-02-13 11:17:14.590000\n",
      "committing chunk 13 of 93 with length 10000 at 2018-02-13 11:17:14.904000\n",
      "committing chunk 14 of 93 with length 10000 at 2018-02-13 11:17:15.259000\n",
      "committing chunk 15 of 93 with length 10000 at 2018-02-13 11:17:15.581000\n",
      "committing chunk 16 of 93 with length 10000 at 2018-02-13 11:17:15.940000\n",
      "committing chunk 17 of 93 with length 10000 at 2018-02-13 11:17:16.268000\n",
      "committing chunk 18 of 93 with length 10000 at 2018-02-13 11:17:16.582000\n",
      "committing chunk 19 of 93 with length 10000 at 2018-02-13 11:17:16.927000\n",
      "committing chunk 20 of 93 with length 10000 at 2018-02-13 11:17:17.351000\n",
      "committing chunk 21 of 93 with length 10000 at 2018-02-13 11:17:17.651000\n",
      "committing chunk 22 of 93 with length 10000 at 2018-02-13 11:17:27.962000\n",
      "committing chunk 23 of 93 with length 10000 at 2018-02-13 11:17:42.222000\n",
      "committing chunk 24 of 93 with length 10000 at 2018-02-13 11:17:54.255000\n",
      "committing chunk 25 of 93 with length 10000 at 2018-02-13 11:18:05.837000\n",
      "committing chunk 26 of 93 with length 10000 at 2018-02-13 11:18:15.782000\n",
      "committing chunk 27 of 93 with length 10000 at 2018-02-13 11:18:24.508000\n",
      "committing chunk 28 of 93 with length 10000 at 2018-02-13 11:18:32.791000\n",
      "committing chunk 29 of 93 with length 10000 at 2018-02-13 11:18:38.279000\n",
      "committing chunk 30 of 93 with length 10000 at 2018-02-13 11:18:45.069000\n",
      "committing chunk 31 of 93 with length 10000 at 2018-02-13 11:18:51.297000\n",
      "committing chunk 32 of 93 with length 10000 at 2018-02-13 11:18:56.560000\n",
      "committing chunk 33 of 93 with length 10000 at 2018-02-13 11:19:02.122000\n",
      "committing chunk 34 of 93 with length 10000 at 2018-02-13 11:19:05.858000\n",
      "committing chunk 35 of 93 with length 10000 at 2018-02-13 11:19:09.844000\n",
      "committing chunk 36 of 93 with length 10000 at 2018-02-13 11:19:12.193000\n",
      "committing chunk 37 of 93 with length 10000 at 2018-02-13 11:19:14.877000\n",
      "committing chunk 38 of 93 with length 10000 at 2018-02-13 11:19:17.594000\n",
      "committing chunk 39 of 93 with length 10000 at 2018-02-13 11:19:20.763000\n",
      "committing chunk 40 of 93 with length 10000 at 2018-02-13 11:19:22.767000\n",
      "committing chunk 41 of 93 with length 10000 at 2018-02-13 11:19:24.743000\n",
      "committing chunk 42 of 93 with length 10000 at 2018-02-13 11:19:26.155000\n",
      "committing chunk 43 of 93 with length 10000 at 2018-02-13 11:19:28.214000\n",
      "committing chunk 44 of 93 with length 10000 at 2018-02-13 11:19:30.198000\n",
      "committing chunk 45 of 93 with length 10000 at 2018-02-13 11:19:31.619000\n",
      "committing chunk 46 of 93 with length 10000 at 2018-02-13 11:19:31.985000\n",
      "committing chunk 47 of 93 with length 10000 at 2018-02-13 11:19:32.356000\n",
      "committing chunk 48 of 93 with length 10000 at 2018-02-13 11:19:32.738000\n",
      "committing chunk 49 of 93 with length 10000 at 2018-02-13 11:19:33.102000\n",
      "committing chunk 50 of 93 with length 10000 at 2018-02-13 11:19:33.468000\n",
      "committing chunk 51 of 93 with length 10000 at 2018-02-13 11:19:33.795000\n",
      "committing chunk 52 of 93 with length 10000 at 2018-02-13 11:19:34.105000\n",
      "committing chunk 53 of 93 with length 10000 at 2018-02-13 11:19:34.445000\n",
      "committing chunk 54 of 93 with length 10000 at 2018-02-13 11:19:34.744000\n",
      "committing chunk 55 of 93 with length 10000 at 2018-02-13 11:19:35.094000\n",
      "committing chunk 56 of 93 with length 10000 at 2018-02-13 11:19:35.363000\n",
      "committing chunk 57 of 93 with length 10000 at 2018-02-13 11:19:35.675000\n",
      "committing chunk 58 of 93 with length 10000 at 2018-02-13 11:19:35.981000\n",
      "committing chunk 59 of 93 with length 10000 at 2018-02-13 11:19:36.266000\n",
      "committing chunk 60 of 93 with length 10000 at 2018-02-13 11:19:36.590000\n",
      "committing chunk 61 of 93 with length 10000 at 2018-02-13 11:19:36.923000\n",
      "committing chunk 62 of 93 with length 10000 at 2018-02-13 11:19:37.223000\n",
      "committing chunk 63 of 93 with length 10000 at 2018-02-13 11:19:37.560000\n",
      "committing chunk 64 of 93 with length 10000 at 2018-02-13 11:19:37.869000\n",
      "committing chunk 65 of 93 with length 10000 at 2018-02-13 11:19:38.237000\n",
      "committing chunk 66 of 93 with length 10000 at 2018-02-13 11:19:38.554000\n",
      "committing chunk 67 of 93 with length 10000 at 2018-02-13 11:19:38.911000\n",
      "committing chunk 68 of 93 with length 10000 at 2018-02-13 11:19:39.271000\n",
      "committing chunk 69 of 93 with length 10000 at 2018-02-13 11:19:39.608000\n",
      "committing chunk 70 of 93 with length 10000 at 2018-02-13 11:19:39.946000\n",
      "committing chunk 71 of 93 with length 10000 at 2018-02-13 11:19:40.254000\n",
      "committing chunk 72 of 93 with length 10000 at 2018-02-13 11:19:40.556000\n",
      "committing chunk 73 of 93 with length 10000 at 2018-02-13 11:19:40.892000\n",
      "committing chunk 74 of 93 with length 10000 at 2018-02-13 11:19:41.221000\n",
      "committing chunk 75 of 93 with length 10000 at 2018-02-13 11:19:41.530000\n",
      "committing chunk 76 of 93 with length 10000 at 2018-02-13 11:19:41.873000\n",
      "committing chunk 77 of 93 with length 10000 at 2018-02-13 11:19:42.201000\n",
      "committing chunk 78 of 93 with length 10000 at 2018-02-13 11:19:42.532000\n",
      "committing chunk 79 of 93 with length 10000 at 2018-02-13 11:19:42.851000\n",
      "committing chunk 80 of 93 with length 10000 at 2018-02-13 11:19:43.269000\n",
      "committing chunk 81 of 93 with length 10000 at 2018-02-13 11:19:43.566000\n",
      "committing chunk 82 of 93 with length 10000 at 2018-02-13 11:19:43.904000\n",
      "committing chunk 83 of 93 with length 10000 at 2018-02-13 11:19:44.244000\n",
      "committing chunk 84 of 93 with length 10000 at 2018-02-13 11:19:44.614000\n",
      "committing chunk 85 of 93 with length 10000 at 2018-02-13 11:19:44.914000\n",
      "committing chunk 86 of 93 with length 10000 at 2018-02-13 11:19:45.285000\n",
      "committing chunk 87 of 93 with length 10000 at 2018-02-13 11:19:45.603000\n",
      "committing chunk 88 of 93 with length 10000 at 2018-02-13 11:19:46.002000\n",
      "committing chunk 89 of 93 with length 10000 at 2018-02-13 11:19:46.273000\n",
      "committing chunk 90 of 93 with length 10000 at 2018-02-13 11:19:46.655000\n",
      "committing chunk 91 of 93 with length 10000 at 2018-02-13 11:19:47.022000\n",
      "committing chunk 92 of 93 with length 10000 at 2018-02-13 11:19:47.379000\n",
      "committing chunk 93 of 93 with length 10000 at 2018-02-13 11:19:47.783000\n",
      "committing last 8225 records at 2018-02-13 11:19:47.795000\n",
      "last chunk in, committing\n",
      "9555 2018-02-13 11:19:58.941000\n"
     ]
    }
   ],
   "source": [
    "do_jw(lawyer_alpha_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1940"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lawyer_insert_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'pk': u'ydwx2zvlymw1evn42shz0xnkh',\n",
       "  'update': 'a7ce962d99434619aba601f47dbacdbe'},\n",
       " {'pk': u'ydwx2zvlymw1evn42shz0xnkh',\n",
       "  'update': 'a7ce962d99434619aba601f47dbacdbe'},\n",
       " {'pk': u'ydwx2zvlymw1evn42shz0xnkh',\n",
       "  'update': 'a7ce962d99434619aba601f47dbacdbe'},\n",
       " {'pk': u'ydwx2zvlymw1evn42shz0xnkh',\n",
       "  'update': 'a7ce962d99434619aba601f47dbacdbe'},\n",
       " {'pk': u'sqwwgr00bku2cidck30u7hm19',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'},\n",
       " {'pk': u'zpyl1v2ylk045lzg6elix2ggs',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'},\n",
       " {'pk': u'sqwwgr00bku2cidck30u7hm19',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'},\n",
       " {'pk': u'zpyl1v2ylk045lzg6elix2ggs',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'},\n",
       " {'pk': u'sqwwgr00bku2cidck30u7hm19',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'},\n",
       " {'pk': u'zpyl1v2ylk045lzg6elix2ggs',\n",
       "  'update': '7d22cc8a645cb97278aa00c3d32a5bd0'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_statements[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [u'Zhengui|Yao', u'Zhengui|Yao', u'Zhengui|Yao', u'Zhengui|Yao'],\n",
       " [u'Zoya V.|Chernina',\n",
       "  u'Zoya V.|Chernina',\n",
       "  u'Zoya V.|Chernina',\n",
       "  u'Zoya V.|Chernina'],\n",
       " [u'Xiaoguang|Gao', u'Xiaoguang|Gao'],\n",
       " [u'Ziad N.|Zarka', u'Ziad N.|Zarka', u'Ziad N.|Zarka', u'Ziad N.|Zarka'],\n",
       " [u'Zachary T.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, nd',\n",
       "  u'Zachary Y.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, II',\n",
       "  u'Zachary T.|Wobensmith',\n",
       "  u'Zachary T.|Wobonsmith, III',\n",
       "  u'Zachary T.|Wohensmith, III',\n",
       "  u'Zachary T.|Wobensmith, Jr.',\n",
       "  u'Zachary T.|Wobonsmith',\n",
       "  u'Zachary T.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, nd',\n",
       "  u'Zachary Y.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, II',\n",
       "  u'Zachary T.|Wobensmith',\n",
       "  u'Zachary T.|Wobonsmith, III',\n",
       "  u'Zachary T.|Wohensmith, III',\n",
       "  u'Zachary T.|Wobensmith, Jr.',\n",
       "  u'Zachary T.|Wobonsmith',\n",
       "  u'Zachary T.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, nd',\n",
       "  u'Zachary Y.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, II',\n",
       "  u'Zachary T.|Wobensmith',\n",
       "  u'Zachary T.|Wobonsmith, III',\n",
       "  u'Zachary T.|Wohensmith, III',\n",
       "  u'Zachary T.|Wobensmith, Jr.',\n",
       "  u'Zachary T.|Wobonsmith',\n",
       "  u'Zachary T.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, nd',\n",
       "  u'Zachary Y.|Wobensmith, III',\n",
       "  u'Zachary T.|Wobensmith, II',\n",
       "  u'Zachary T.|Wobensmith',\n",
       "  u'Zachary T.|Wobonsmith, III',\n",
       "  u'Zachary T.|Wohensmith, III',\n",
       "  u'Zachary T.|Wobensmith, Jr.',\n",
       "  u'Zachary T.|Wobonsmith'],\n",
       " [u'Xin|Wen', u'Xin Wen', u'Xin|Wen', u'Xin Wen'],\n",
       " [u'X|Kong', u'X|Kong'],\n",
       " [u'Zollinger &#x; Burleson Ltd.',\n",
       "  u'Zollinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Burleson',\n",
       "  u'Zollinger & Burleson',\n",
       "  u'Zollinger &#x; Bucleson Ltd',\n",
       "  u'Zollinger &#x; Burleson, III',\n",
       "  u'Zollinger &#x; Bueleson Ltd.',\n",
       "  u'Zollinger &#x; Burkeson Ltd',\n",
       "  u'Zollinger &#x; Burlesoa Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd',\n",
       "  u'Zallinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Bucleson Ltd.',\n",
       "  u'Zolliger &#x; Buckson Ltd.',\n",
       "  u'Zollinger &#x; Burlem Ltd.',\n",
       "  u'Zollinger &#x; Burleron Ltd.',\n",
       "  u'Zollinger & Burleson Ltd.',\n",
       "  u'Zollinger &#x; Buclesan Ltd',\n",
       "  u'Zollinger &#x; Burleson Ltd.',\n",
       "  u'Zollinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Burleson',\n",
       "  u'Zollinger & Burleson',\n",
       "  u'Zollinger &#x; Bucleson Ltd',\n",
       "  u'Zollinger &#x; Burleson, III',\n",
       "  u'Zollinger &#x; Bueleson Ltd.',\n",
       "  u'Zollinger &#x; Burkeson Ltd',\n",
       "  u'Zollinger &#x; Burlesoa Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd',\n",
       "  u'Zallinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Bucleson Ltd.',\n",
       "  u'Zolliger &#x; Buckson Ltd.',\n",
       "  u'Zollinger &#x; Burlem Ltd.',\n",
       "  u'Zollinger &#x; Burleron Ltd.',\n",
       "  u'Zollinger & Burleson Ltd.',\n",
       "  u'Zollinger &#x; Buclesan Ltd',\n",
       "  u'Zollinger &#x; Burleson Ltd.',\n",
       "  u'Zollinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Burleson',\n",
       "  u'Zollinger & Burleson',\n",
       "  u'Zollinger &#x; Bucleson Ltd',\n",
       "  u'Zollinger &#x; Burleson, III',\n",
       "  u'Zollinger &#x; Bueleson Ltd.',\n",
       "  u'Zollinger &#x; Burkeson Ltd',\n",
       "  u'Zollinger &#x; Burlesoa Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd',\n",
       "  u'Zallinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Bucleson Ltd.',\n",
       "  u'Zolliger &#x; Buckson Ltd.',\n",
       "  u'Zollinger &#x; Burlem Ltd.',\n",
       "  u'Zollinger &#x; Burleron Ltd.',\n",
       "  u'Zollinger & Burleson Ltd.',\n",
       "  u'Zollinger &#x; Buclesan Ltd',\n",
       "  u'Zollinger &#x; Burleson Ltd.',\n",
       "  u'Zollinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Burleson',\n",
       "  u'Zollinger & Burleson',\n",
       "  u'Zollinger &#x; Bucleson Ltd',\n",
       "  u'Zollinger &#x; Burleson, III',\n",
       "  u'Zollinger &#x; Bueleson Ltd.',\n",
       "  u'Zollinger &#x; Burkeson Ltd',\n",
       "  u'Zollinger &#x; Burlesoa Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd.',\n",
       "  u'Zollinger &#x; Burleson, Ltd',\n",
       "  u'Zallinger &#x; Burleson Ltd',\n",
       "  u'Zollinger &#x; Bucleson Ltd.',\n",
       "  u'Zolliger &#x; Buckson Ltd.',\n",
       "  u'Zollinger &#x; Burlem Ltd.',\n",
       "  u'Zollinger &#x; Burleron Ltd.',\n",
       "  u'Zollinger & Burleson Ltd.',\n",
       "  u'Zollinger &#x; Buclesan Ltd'],\n",
       " [u'Zimmermann &#x; Partner',\n",
       "  u'Zimmermann &#x; Partner',\n",
       "  u'Zimmermann &#x; Partner',\n",
       "  u'Zimmermann &#x; Partner']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[results[key] for key in results.keys()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
